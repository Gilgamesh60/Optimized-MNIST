{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gilgamesh60/Optimized-MNIST/blob/main/Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TDLtCu4UpCXi"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, I'll load the MNIST multidigit classification model I created in the previous notebook with PyTorch. Then we will use this model for predicting the class of a single input image.\n",
        "\n",
        "\n",
        "For implementing this notebook, please download the `main.py` and `checkpoint.pt` files that I have shared.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "-aXxki9SHrN5"
      }
    },
    {
      "metadata": {
        "id": "rsowkYRdpCXj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "accedff8-b537-4807-d53a-6e63bfc34c61"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# upload external file before import\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d0ee7e1-5666-4f8e-8a59-3ee620e70055\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2d0ee7e1-5666-4f8e-8a59-3ee620e70055\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving checkpoint.pt to checkpoint.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Model architecture.\n",
        "\n",
        "To make things more concise here,I created a seperate python file containing CNN model and training function.I have named the file `main.py`. Importing this, we can easily create a CNN network with and train the network using train function."
      ],
      "metadata": {
        "id": "s-Sz0HKhvGXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('main.py','wb').write(src)\n",
        "import main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "yv47IEjWssTZ",
        "outputId": "cdf97e10-aa28-473d-8568-1b09a8310287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-11aad3c6-f01c-47de-b437-94684d16679a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-11aad3c6-f01c-47de-b437-94684d16679a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving main.py to main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will import our CNN model and training function to train that CNN model.\n",
        "from main import ModelM3\n",
        "\n",
        "\n",
        "# Our train function takes these inputs : EPOCHS,optimizer,model,trainloader,testloader,DEVICE\n",
        "#It will give the loss and accuract at each Epoch as an output.\n",
        "from main import train"
      ],
      "metadata": {
        "id": "H9twtrYnwdF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and formatting MNIST\n",
        "\n",
        "In order to test our model out, I decided to use the ready made MNIST dataset stored pytorch datasets module. We will see the accuracy of our loaded model on this database."
      ],
      "metadata": {
        "id": "4tCd1AnSxEQI"
      }
    },
    {
      "metadata": {
        "id": "NblgZ0v1pCXm"
      },
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "\n",
        "Here we can see one of the images.We will see how our model predicts its class\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ktJ4rU-cFHkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testset = datasets.MNIST('MNIST_data/', download=True) # we will use this dataset for single image prediction \n",
        "plt.imshow(testset[1][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "c8pK4m0nE-jP",
        "outputId": "503ca399-2c18-4b88-d662-8a38cd9d8bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f80fe889970>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 251,
              "height": 248
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pCQpHoC_pCX3"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the model\n",
        "\n",
        "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers. Same goes for optimizer `state_dict`. In the previous noteboolk I saved both of them in a `checkpoint.pt` file. By loading the file we will be able to configure our model for digit classification/recognition tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint=torch.load('/content/checkpoint.pt')"
      ],
      "metadata": {
        "id": "IhI4__XDRfJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=ModelM3()\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "learning_rate=0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhASt1nNUwNj",
        "outputId": "52d9268c-0d91-4e9d-ab9a-ec32b3b77386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelM3(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv2_bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv4_bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5): Conv2d(80, 96, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv5_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv6): Conv2d(96, 112, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv6_bn): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv7): Conv2d(112, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv7_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv8): Conv2d(128, 144, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv8_bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv9): Conv2d(144, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv9_bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv10): Conv2d(160, 176, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv10_bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (drop1): Dropout2d(p=0.30000000000000004, inplace=False)\n",
              "  (fc1): Linear(in_features=11264, out_features=96, bias=True)\n",
              "  (drop2): Dropout2d(p=0.2, inplace=False)\n",
              "  (fc2): Linear(in_features=96, out_features=10, bias=True)\n",
              "  (fc1_bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "lBqSgQCNpCX4",
        "outputId": "19c7fb36-423e-4e2e-dcd0-62810c0cca98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Our model: \\n\\n\", model, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model: \n",
            "\n",
            " ModelM3(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv2_bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv4_bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(80, 96, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv5_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv6): Conv2d(96, 112, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv6_bn): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv7): Conv2d(112, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv7_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv8): Conv2d(128, 144, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv8_bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv9): Conv2d(144, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv9_bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv10): Conv2d(160, 176, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv10_bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (drop1): Dropout2d(p=0.30000000000000004, inplace=False)\n",
            "  (fc1): Linear(in_features=11264, out_features=96, bias=True)\n",
            "  (drop2): Dropout2d(p=0.2, inplace=False)\n",
            "  (fc2): Linear(in_features=96, out_features=10, bias=True)\n",
            "  (fc1_bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " odict_keys(['conv1.weight', 'conv1_bn.weight', 'conv1_bn.bias', 'conv1_bn.running_mean', 'conv1_bn.running_var', 'conv1_bn.num_batches_tracked', 'conv2.weight', 'conv2_bn.weight', 'conv2_bn.bias', 'conv2_bn.running_mean', 'conv2_bn.running_var', 'conv2_bn.num_batches_tracked', 'conv3.weight', 'conv3_bn.weight', 'conv3_bn.bias', 'conv3_bn.running_mean', 'conv3_bn.running_var', 'conv3_bn.num_batches_tracked', 'conv4.weight', 'conv4_bn.weight', 'conv4_bn.bias', 'conv4_bn.running_mean', 'conv4_bn.running_var', 'conv4_bn.num_batches_tracked', 'conv5.weight', 'conv5_bn.weight', 'conv5_bn.bias', 'conv5_bn.running_mean', 'conv5_bn.running_var', 'conv5_bn.num_batches_tracked', 'conv6.weight', 'conv6_bn.weight', 'conv6_bn.bias', 'conv6_bn.running_mean', 'conv6_bn.running_var', 'conv6_bn.num_batches_tracked', 'conv7.weight', 'conv7_bn.weight', 'conv7_bn.bias', 'conv7_bn.running_mean', 'conv7_bn.running_var', 'conv7_bn.num_batches_tracked', 'conv8.weight', 'conv8_bn.weight', 'conv8_bn.bias', 'conv8_bn.running_mean', 'conv8_bn.running_var', 'conv8_bn.num_batches_tracked', 'conv9.weight', 'conv9_bn.weight', 'conv9_bn.bias', 'conv9_bn.running_mean', 'conv9_bn.running_var', 'conv9_bn.num_batches_tracked', 'conv10.weight', 'conv10_bn.weight', 'conv10_bn.bias', 'conv10_bn.running_mean', 'conv10_bn.running_var', 'conv10_bn.num_batches_tracked', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc1_bn.weight', 'fc1_bn.bias', 'fc1_bn.running_mean', 'fc1_bn.running_var', 'fc1_bn.num_batches_tracked'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvd4KsCk6vKx",
        "outputId": "5c6ca3ee-00b5-469c-a388-4996fafdf4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelM3(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv2_bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv4_bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5): Conv2d(80, 96, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv5_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv6): Conv2d(96, 112, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv6_bn): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv7): Conv2d(112, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv7_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv8): Conv2d(128, 144, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv8_bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv9): Conv2d(144, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv9_bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv10): Conv2d(160, 176, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "  (conv10_bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (drop1): Dropout2d(p=0.30000000000000004, inplace=False)\n",
              "  (fc1): Linear(in_features=11264, out_features=96, bias=True)\n",
              "  (drop2): Dropout2d(p=0.2, inplace=False)\n",
              "  (fc2): Linear(in_features=96, out_features=10, bias=True)\n",
              "  (fc1_bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an Image Classifier\n",
        "\n",
        "\n",
        "We will finally use our generated model to get prdictions for a single image"
      ],
      "metadata": {
        "id": "xWrrUkY2IgTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this will take an input image and give a output tensor for prediction\n",
        "def transform_image(img):\n",
        "    my_transforms = transforms.Compose([\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    image = np.asarray(img)\n",
        "    return my_transforms(image).unsqueeze(0)"
      ],
      "metadata": {
        "id": "7Q1O9GCZ64bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outputs the predicted class and confidence in the prediction for input image.\n",
        "def get_prediction(img):\n",
        "    tensor = transform_image(img)\n",
        "    tensor=tensor.to(device)\n",
        "    output = model.forward(tensor)\n",
        "     \n",
        "    probs = torch.nn.functional.softmax(output, dim=1)\n",
        "    conf, classes = torch.max(probs, 1)\n",
        "    return conf.item(), classes.item()"
      ],
      "metadata": {
        "id": "RbrKZX2vAMN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Conclusion : \")\n",
        "print(f\"The label for this image is {testset[1][1]}\")\n",
        "print(f\"The prediction given by our model is {get_prediction(testset[1][0])[1]} and confidence in the prediction is {get_prediction(testset[1][0])[0]}\")\n",
        "print(\"This is the image that we predicted the label of :\")\n",
        "plt.imshow(np.asarray(testset[1][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "WJ20bxDZAdZE",
        "outputId": "99a34c72-efb4-40ee-b4c7-6d77c8c75913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conclusion : \n",
            "The label for this image is 0\n",
            "The prediction given by our model is 0 and confidence in the prediction is 0.9997583031654358\n",
            "This is the image that we predicted the label of :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f80fe8b6760>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 251,
              "height": 248
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will work for any PIL image of the size (28,28)"
      ],
      "metadata": {
        "id": "bif8D2jPZjEu"
      }
    }
  ]
}