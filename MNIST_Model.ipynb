{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gilgamesh60/Optimized-MNIST/blob/main/MNIST_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook I have built a model for implementing a multi digit classification on the given MNIST database.\n",
        "\n",
        "For this I decided to use a deep CNN model . I have explained the detailed architecture of this CNN later . In order to get the maximum accuracy possible I have also implemented hyperparameter tuning on this CNN using Optuna package."
      ],
      "metadata": {
        "id": "5xyf9zm5cJJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impoting Libraries"
      ],
      "metadata": {
        "id": "P4it2-I3e8cJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhwhTJkIPK7b"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3v1XgTwxmWO"
      },
      "outputs": [],
      "source": [
        "from optuna.samplers import TPESampler\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the MNIST dataset\n",
        "\n",
        "This is the pre-built code in the assignment.This code will create processed and raw image folders. In the processed folder,we have train and test .pt files.These files contain MNIST database in the form of images and labels(for both train and test datasets). We can use torch.load to get this in the form of image tensors and labels from the beforementioned .pt files"
      ],
      "metadata": {
        "id": "Kgy4EAB6fFGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drKOrjNHPg1e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "\n",
        "\n",
        "class MNISTM(VisionDataset):\n",
        "    \"\"\"MNIST-M Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    resources = [\n",
        "        ('https://github.com/liyxi/mnist-m/releases/download/data/mnist_m_train.pt.tar.gz',\n",
        "         '191ed53db9933bd85cc9700558847391'),\n",
        "        ('https://github.com/liyxi/mnist-m/releases/download/data/mnist_m_test.pt.tar.gz',\n",
        "         'e11cb4d7fff76d7ec588b1134907db59')\n",
        "    ]\n",
        "\n",
        "    training_file = \"mnist_m_train.pt\"\n",
        "    test_file = \"mnist_m_test.pt\"\n",
        "    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',\n",
        "               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
        "\n",
        "    @property\n",
        "    def train_labels(self):\n",
        "        warnings.warn(\"train_labels has been renamed targets\")\n",
        "        return self.targets\n",
        "\n",
        "    @property\n",
        "    def test_labels(self):\n",
        "        warnings.warn(\"test_labels has been renamed targets\")\n",
        "        return self.targets\n",
        "\n",
        "    @property\n",
        "    def train_data(self):\n",
        "        warnings.warn(\"train_data has been renamed data\")\n",
        "        return self.data\n",
        "\n",
        "    @property\n",
        "    def test_data(self):\n",
        "        warnings.warn(\"test_data has been renamed data\")\n",
        "        return self.data\n",
        "\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "        \"\"\"Init MNIST-M dataset.\"\"\"\n",
        "        super(MNISTM, self).__init__(root, transform, target_transform=target_transform)\n",
        "\n",
        "        self.train = train\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError(\"Dataset not found.\" +\n",
        "                               \" You can use download=True to download it\")\n",
        "\n",
        "        if self.train:\n",
        "            data_file = self.training_file\n",
        "        else:\n",
        "            data_file = self.test_file\n",
        "\n",
        "        print(os.path.join(self.processed_folder, data_file))\n",
        "\n",
        "        self.data, self.targets = torch.load(os.path.join(self.processed_folder, data_file))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get images and target for data loader.\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.data[index], int(self.targets[index])\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img.squeeze().numpy(), 'L')\n",
        "        img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return size of dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    @property\n",
        "    def raw_folder(self):\n",
        "        return os.path.join(self.root, self.__class__.__name__, 'raw')\n",
        "\n",
        "    @property\n",
        "    def processed_folder(self):\n",
        "        return os.path.join(self.root, self.__class__.__name__, 'processed')\n",
        "\n",
        "    @property\n",
        "    def class_to_idx(self):\n",
        "        return {_class: i for i, _class in enumerate(self.classes)}\n",
        "\n",
        "    def _check_exists(self):\n",
        "        return (os.path.exists(os.path.join(self.processed_folder, self.training_file)) and\n",
        "                os.path.exists(os.path.join(self.processed_folder, self.test_file)))\n",
        "\n",
        "    def download(self):\n",
        "        \"\"\"Download the MNIST-M data.\"\"\"\n",
        "\n",
        "        if self._check_exists():\n",
        "            return\n",
        "\n",
        "        os.makedirs(self.raw_folder, exist_ok=True)\n",
        "        os.makedirs(self.processed_folder, exist_ok=True)\n",
        "\n",
        "        # download files\n",
        "        for url, md5 in self.resources:\n",
        "            filename = url.rpartition('/')[2]\n",
        "            download_and_extract_archive(url, download_root=self.raw_folder,\n",
        "                                         extract_root=self.processed_folder,\n",
        "                                         filename=filename, md5=md5)\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return \"Split: {}\".format(\"Train\" if self.train is True else \"Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading MNIST dataset."
      ],
      "metadata": {
        "id": "r09zD7aliMbp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "617c4cc74d004ef382800e5600a831c8",
            "befe02bf9d3a48e0b21a662a394ded31",
            "9a3f8f5a600543dea4f7503eab169192",
            "f64a9a3ad6994d3a8f649f7aadfec484",
            "5d6fa4c2ff084a91a530d4a341c5fc62",
            "a51a8559249249e1b85d0043b925d0b3",
            "22eb1b86b5a14dd6ad7868c697820f99",
            "80e9f608fda94193b487b0233fe2f1f0",
            "2f85549714af4f4b9d0116e3c614614e",
            "005ecfa5731e48eb895b2bcdfe41bdce",
            "866ea2b99508436b99d74d83e0a28237",
            "8ca69999cbb443b3ba5c167cd004d774",
            "04d79a3b1d3c43e9aec7b8b486356c3c",
            "0080f9ff5e124d1f853ad84998862177",
            "a550b262059f4fd884cfbb5ddb861988",
            "c93e810133a14a268c416f6d572e2f01",
            "ca2b4c09a3754fd182975cb838ad6b27",
            "cb82bfd4a40d484790cb5f79276d4a12",
            "cd3079cbc19f446d8d6dfa9b832e3772",
            "fcb1ee9fc47d41ad9271abff8d25ba69",
            "850abcd3bdc744c69fb4f282af5e20ad",
            "359f20001b2043c0b2723b08bdc5b337"
          ]
        },
        "id": "867tRXoXPshV",
        "outputId": "e82d9941-90a7-40c8-bea0-f68f8b690273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/302251211/1e1da080-0994-11eb-9dc3-a59cab416836?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221206T042852Z&X-Amz-Expires=300&X-Amz-Signature=0e1aa094d35f87b73ba67738c280cad597952679dcaaa33d1da072b108560d96&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=302251211&response-content-disposition=attachment%3B%20filename%3Dmnist_m_train.pt.tar.gz&response-content-type=application%2Foctet-stream to MNIST/raw/train-images-idx3-ubyte/MNISTM/raw/mnist_m_train.pt.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/112143175 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "617c4cc74d004ef382800e5600a831c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/raw/train-images-idx3-ubyte/MNISTM/raw/mnist_m_train.pt.tar.gz to MNIST/raw/train-images-idx3-ubyte/MNISTM/processed\n",
            "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/302251211/d39c2400-0993-11eb-9b8e-fbae8eb1b3a9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221206T042919Z&X-Amz-Expires=300&X-Amz-Signature=7daf47af26fcec621bb338b42ca5baafb1dbdc69b16f8b0b0ec3b75e97b7679b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=302251211&response-content-disposition=attachment%3B%20filename%3Dmnist_m_test.pt.tar.gz&response-content-type=application%2Foctet-stream to MNIST/raw/train-images-idx3-ubyte/MNISTM/raw/mnist_m_test.pt.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/18663594 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ca69999cbb443b3ba5c167cd004d774"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/raw/train-images-idx3-ubyte/MNISTM/raw/mnist_m_test.pt.tar.gz to MNIST/raw/train-images-idx3-ubyte/MNISTM/processed\n",
            "Done!\n",
            "MNIST/raw/train-images-idx3-ubyte/MNISTM/processed/mnist_m_train.pt\n"
          ]
        }
      ],
      "source": [
        "p1=MNISTM('MNIST/raw/train-images-idx3-ubyte',download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvLWgxo1ngFC"
      },
      "outputs": [],
      "source": [
        "#Downloading the processed train and test .pt files\n",
        "p1.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "599tteeyo92e"
      },
      "outputs": [],
      "source": [
        "#Loading the .pt files and store them in train_data and test_data\n",
        "train_data=torch.load('/content/MNIST/raw/train-images-idx3-ubyte/MNISTM/processed/mnist_m_train.pt')\n",
        "test_data=torch.load('/content/MNIST/raw/train-images-idx3-ubyte/MNISTM/processed/mnist_m_test.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnkh1-qJi59o",
        "outputId": "a6ff2b87-f8ec-4a76-e01d-914e06ca92fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[[ 60,  77, 133],\n",
              "           [ 61,  78, 134],\n",
              "           [ 62,  79, 135],\n",
              "           ...,\n",
              "           [ 57,  76, 132],\n",
              "           [ 57,  76, 132],\n",
              "           [ 57,  77, 130]],\n",
              " \n",
              "          [[ 60,  77, 133],\n",
              "           [ 61,  78, 134],\n",
              "           [ 61,  78, 134],\n",
              "           ...,\n",
              "           [ 57,  76, 132],\n",
              "           [ 57,  76, 132],\n",
              "           [ 57,  76, 132]],\n",
              " \n",
              "          [[ 61,  78, 134],\n",
              "           [ 61,  78, 134],\n",
              "           [ 61,  78, 134],\n",
              "           ...,\n",
              "           [ 56,  75, 131],\n",
              "           [ 56,  75, 131],\n",
              "           [ 57,  76, 132]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[ 57,  76, 134],\n",
              "           [ 57,  77, 130],\n",
              "           [ 57,  77, 130],\n",
              "           ...,\n",
              "           [  6,  34,  35],\n",
              "           [  6,  35,  33],\n",
              "           [  7,  36,  32]],\n",
              " \n",
              "          [[ 58,  77, 135],\n",
              "           [ 58,  77, 133],\n",
              "           [ 57,  77, 130],\n",
              "           ...,\n",
              "           [  9,  37,  38],\n",
              "           [  8,  37,  35],\n",
              "           [  8,  37,  35]],\n",
              " \n",
              "          [[ 57,  73, 133],\n",
              "           [ 62,  78, 137],\n",
              "           [ 58,  75, 131],\n",
              "           ...,\n",
              "           [  8,  37,  33],\n",
              "           [  8,  37,  35],\n",
              "           [  8,  37,  35]]],\n",
              " \n",
              " \n",
              "         [[[255, 208, 153],\n",
              "           [255, 207, 151],\n",
              "           [255, 205, 150],\n",
              "           ...,\n",
              "           [255, 205, 144],\n",
              "           [255, 206, 145],\n",
              "           [253, 203, 142]],\n",
              " \n",
              "          [[255, 208, 152],\n",
              "           [253, 206, 150],\n",
              "           [253, 206, 150],\n",
              "           ...,\n",
              "           [255, 206, 143],\n",
              "           [255, 207, 144],\n",
              "           [253, 205, 143]],\n",
              " \n",
              "          [[255, 206, 150],\n",
              "           [253, 206, 150],\n",
              "           [254, 207, 151],\n",
              "           ...,\n",
              "           [253, 197, 136],\n",
              "           [250, 199, 136],\n",
              "           [254, 206, 144]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[255, 198, 144],\n",
              "           [255, 193, 141],\n",
              "           [255, 191, 139],\n",
              "           ...,\n",
              "           [255, 199, 148],\n",
              "           [253, 197, 146],\n",
              "           [255, 199, 150]],\n",
              " \n",
              "          [[255, 207, 150],\n",
              "           [255, 205, 148],\n",
              "           [253, 202, 145],\n",
              "           ...,\n",
              "           [255, 203, 151],\n",
              "           [250, 197, 145],\n",
              "           [251, 198, 146]],\n",
              " \n",
              "          [[255, 205, 148],\n",
              "           [255, 208, 150],\n",
              "           [255, 207, 149],\n",
              "           ...,\n",
              "           [255, 202, 150],\n",
              "           [255, 202, 150],\n",
              "           [255, 202, 150]]],\n",
              " \n",
              " \n",
              "         [[[ 68,  56,  42],\n",
              "           [116,  99,  83],\n",
              "           [155, 137, 117],\n",
              "           ...,\n",
              "           [239, 214,  88],\n",
              "           [238, 213,  86],\n",
              "           [240, 213,  82]],\n",
              " \n",
              "          [[111, 103,  90],\n",
              "           [ 87,  75,  59],\n",
              "           [ 95,  80,  61],\n",
              "           ...,\n",
              "           [231, 204,  91],\n",
              "           [233, 204,  87],\n",
              "           [233, 203,  81]],\n",
              " \n",
              "          [[118, 115, 106],\n",
              "           [108, 102,  90],\n",
              "           [ 79,  72,  56],\n",
              "           ...,\n",
              "           [235, 209, 112],\n",
              "           [221, 194,  89],\n",
              "           [222, 192,  80]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[ 60,  60,  60],\n",
              "           [ 60,  57,  52],\n",
              "           [ 59,  52,  44],\n",
              "           ...,\n",
              "           [ 91,  78,  46],\n",
              "           [120,  97,  55],\n",
              "           [159, 130,  72]],\n",
              " \n",
              "          [[ 63,  62,  60],\n",
              "           [ 59,  54,  50],\n",
              "           [ 57,  50,  42],\n",
              "           ...,\n",
              "           [115,  92,  58],\n",
              "           [140, 109,  63],\n",
              "           [164, 133,  68]],\n",
              " \n",
              "          [[ 68,  63,  59],\n",
              "           [ 62,  55,  49],\n",
              "           [ 62,  51,  45],\n",
              "           ...,\n",
              "           [113,  89,  51],\n",
              "           [137, 107,  55],\n",
              "           [171, 141,  71]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[ 86,  82,  45],\n",
              "           [ 87,  84,  49],\n",
              "           [255, 255, 227],\n",
              "           ...,\n",
              "           [136, 119,  89],\n",
              "           [133, 117,  83],\n",
              "           [129, 113,  79]],\n",
              " \n",
              "          [[168, 162, 126],\n",
              "           [157, 153, 118],\n",
              "           [255, 255, 227],\n",
              "           ...,\n",
              "           [117, 100,  72],\n",
              "           [118, 101,  71],\n",
              "           [135, 119,  86]],\n",
              " \n",
              "          [[171, 162, 129],\n",
              "           [171, 165, 133],\n",
              "           [167, 160, 132],\n",
              "           ...,\n",
              "           [139, 124,  95],\n",
              "           [150, 133, 105],\n",
              "           [142, 125,  97]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[178, 161, 118],\n",
              "           [180, 158, 117],\n",
              "           [180, 155, 115],\n",
              "           ...,\n",
              "           [158, 111,  81],\n",
              "           [160, 117,  85],\n",
              "           [180, 139, 107]],\n",
              " \n",
              "          [[156, 141, 100],\n",
              "           [175, 153, 112],\n",
              "           [190, 162, 123],\n",
              "           ...,\n",
              "           [201, 152, 122],\n",
              "           [184, 141, 109],\n",
              "           [186, 145, 113]],\n",
              " \n",
              "          [[162, 147, 104],\n",
              "           [167, 145, 104],\n",
              "           [178, 150, 111],\n",
              "           ...,\n",
              "           [207, 160, 132],\n",
              "           [204, 159, 130],\n",
              "           [207, 164, 132]]],\n",
              " \n",
              " \n",
              "         [[[168, 140, 116],\n",
              "           [149, 123,  98],\n",
              "           [156, 130, 105],\n",
              "           ...,\n",
              "           [163, 137, 110],\n",
              "           [132, 104,  80],\n",
              "           [158, 132, 107]],\n",
              " \n",
              "          [[164, 136, 112],\n",
              "           [143, 115,  91],\n",
              "           [157, 129, 105],\n",
              "           ...,\n",
              "           [162, 136, 109],\n",
              "           [135, 109,  82],\n",
              "           [157, 131, 106]],\n",
              " \n",
              "          [[158, 128, 104],\n",
              "           [150, 120,  96],\n",
              "           [148, 120,  96],\n",
              "           ...,\n",
              "           [150, 126,  98],\n",
              "           [156, 132, 104],\n",
              "           [165, 141, 113]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[168, 143, 123],\n",
              "           [138, 113,  93],\n",
              "           [145, 120, 100],\n",
              "           ...,\n",
              "           [146, 120,  97],\n",
              "           [152, 124, 102],\n",
              "           [133, 107,  84]],\n",
              " \n",
              "          [[174, 149, 129],\n",
              "           [134, 109,  89],\n",
              "           [141, 116,  96],\n",
              "           ...,\n",
              "           [124,  98,  75],\n",
              "           [153, 122, 101],\n",
              "           [148, 120,  98]],\n",
              " \n",
              "          [[170, 145, 125],\n",
              "           [129, 104,  84],\n",
              "           [126, 101,  81],\n",
              "           ...,\n",
              "           [130, 103,  82],\n",
              "           [169, 139, 115],\n",
              "           [168, 140, 118]]],\n",
              " \n",
              " \n",
              "         [[[ 46,  55,  50],\n",
              "           [ 46,  55,  50],\n",
              "           [ 52,  61,  56],\n",
              "           ...,\n",
              "           [ 36,  45,  44],\n",
              "           [ 36,  46,  47],\n",
              "           [ 36,  46,  45]],\n",
              " \n",
              "          [[ 42,  46,  45],\n",
              "           [ 46,  50,  49],\n",
              "           [ 50,  54,  53],\n",
              "           ...,\n",
              "           [ 36,  45,  42],\n",
              "           [ 35,  44,  43],\n",
              "           [ 35,  44,  41]],\n",
              " \n",
              "          [[ 50,  56,  56],\n",
              "           [ 40,  46,  44],\n",
              "           [ 42,  48,  46],\n",
              "           ...,\n",
              "           [ 37,  46,  43],\n",
              "           [ 37,  46,  43],\n",
              "           [ 37,  46,  43]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[ 66, 101, 107],\n",
              "           [ 70, 105, 111],\n",
              "           [ 74, 107, 114],\n",
              "           ...,\n",
              "           [ 71, 107, 105],\n",
              "           [ 72, 106, 107],\n",
              "           [ 71, 105, 106]],\n",
              " \n",
              "          [[ 60,  95, 101],\n",
              "           [ 64,  99, 105],\n",
              "           [ 68, 101, 108],\n",
              "           ...,\n",
              "           [ 69, 105, 103],\n",
              "           [ 70, 104, 105],\n",
              "           [ 69, 103, 104]],\n",
              " \n",
              "          [[ 56,  93, 101],\n",
              "           [ 59,  96, 104],\n",
              "           [ 65, 100, 106],\n",
              "           ...,\n",
              "           [ 68, 104, 104],\n",
              "           [ 69, 103, 104],\n",
              "           [ 69, 103, 104]]]], dtype=torch.uint8),\n",
              " tensor([5, 0, 4,  ..., 5, 6, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see we have two seperate tensors. One with the collection of all the image arrays and other with the collection of all the corresponding labels. So we need to convert this into suitable format."
      ],
      "metadata": {
        "id": "MvNjV7wLs7Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first element of train_data is the collection of all image arrays\n",
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fr1yaWKmxun",
        "outputId": "f9c326f2-a64c-4168-a175-f2fac0a89924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 60,  77, 133],\n",
              "          [ 61,  78, 134],\n",
              "          [ 62,  79, 135],\n",
              "          ...,\n",
              "          [ 57,  76, 132],\n",
              "          [ 57,  76, 132],\n",
              "          [ 57,  77, 130]],\n",
              "\n",
              "         [[ 60,  77, 133],\n",
              "          [ 61,  78, 134],\n",
              "          [ 61,  78, 134],\n",
              "          ...,\n",
              "          [ 57,  76, 132],\n",
              "          [ 57,  76, 132],\n",
              "          [ 57,  76, 132]],\n",
              "\n",
              "         [[ 61,  78, 134],\n",
              "          [ 61,  78, 134],\n",
              "          [ 61,  78, 134],\n",
              "          ...,\n",
              "          [ 56,  75, 131],\n",
              "          [ 56,  75, 131],\n",
              "          [ 57,  76, 132]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 57,  76, 134],\n",
              "          [ 57,  77, 130],\n",
              "          [ 57,  77, 130],\n",
              "          ...,\n",
              "          [  6,  34,  35],\n",
              "          [  6,  35,  33],\n",
              "          [  7,  36,  32]],\n",
              "\n",
              "         [[ 58,  77, 135],\n",
              "          [ 58,  77, 133],\n",
              "          [ 57,  77, 130],\n",
              "          ...,\n",
              "          [  9,  37,  38],\n",
              "          [  8,  37,  35],\n",
              "          [  8,  37,  35]],\n",
              "\n",
              "         [[ 57,  73, 133],\n",
              "          [ 62,  78, 137],\n",
              "          [ 58,  75, 131],\n",
              "          ...,\n",
              "          [  8,  37,  33],\n",
              "          [  8,  37,  35],\n",
              "          [  8,  37,  35]]],\n",
              "\n",
              "\n",
              "        [[[255, 208, 153],\n",
              "          [255, 207, 151],\n",
              "          [255, 205, 150],\n",
              "          ...,\n",
              "          [255, 205, 144],\n",
              "          [255, 206, 145],\n",
              "          [253, 203, 142]],\n",
              "\n",
              "         [[255, 208, 152],\n",
              "          [253, 206, 150],\n",
              "          [253, 206, 150],\n",
              "          ...,\n",
              "          [255, 206, 143],\n",
              "          [255, 207, 144],\n",
              "          [253, 205, 143]],\n",
              "\n",
              "         [[255, 206, 150],\n",
              "          [253, 206, 150],\n",
              "          [254, 207, 151],\n",
              "          ...,\n",
              "          [253, 197, 136],\n",
              "          [250, 199, 136],\n",
              "          [254, 206, 144]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[255, 198, 144],\n",
              "          [255, 193, 141],\n",
              "          [255, 191, 139],\n",
              "          ...,\n",
              "          [255, 199, 148],\n",
              "          [253, 197, 146],\n",
              "          [255, 199, 150]],\n",
              "\n",
              "         [[255, 207, 150],\n",
              "          [255, 205, 148],\n",
              "          [253, 202, 145],\n",
              "          ...,\n",
              "          [255, 203, 151],\n",
              "          [250, 197, 145],\n",
              "          [251, 198, 146]],\n",
              "\n",
              "         [[255, 205, 148],\n",
              "          [255, 208, 150],\n",
              "          [255, 207, 149],\n",
              "          ...,\n",
              "          [255, 202, 150],\n",
              "          [255, 202, 150],\n",
              "          [255, 202, 150]]],\n",
              "\n",
              "\n",
              "        [[[ 68,  56,  42],\n",
              "          [116,  99,  83],\n",
              "          [155, 137, 117],\n",
              "          ...,\n",
              "          [239, 214,  88],\n",
              "          [238, 213,  86],\n",
              "          [240, 213,  82]],\n",
              "\n",
              "         [[111, 103,  90],\n",
              "          [ 87,  75,  59],\n",
              "          [ 95,  80,  61],\n",
              "          ...,\n",
              "          [231, 204,  91],\n",
              "          [233, 204,  87],\n",
              "          [233, 203,  81]],\n",
              "\n",
              "         [[118, 115, 106],\n",
              "          [108, 102,  90],\n",
              "          [ 79,  72,  56],\n",
              "          ...,\n",
              "          [235, 209, 112],\n",
              "          [221, 194,  89],\n",
              "          [222, 192,  80]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 60,  60,  60],\n",
              "          [ 60,  57,  52],\n",
              "          [ 59,  52,  44],\n",
              "          ...,\n",
              "          [ 91,  78,  46],\n",
              "          [120,  97,  55],\n",
              "          [159, 130,  72]],\n",
              "\n",
              "         [[ 63,  62,  60],\n",
              "          [ 59,  54,  50],\n",
              "          [ 57,  50,  42],\n",
              "          ...,\n",
              "          [115,  92,  58],\n",
              "          [140, 109,  63],\n",
              "          [164, 133,  68]],\n",
              "\n",
              "         [[ 68,  63,  59],\n",
              "          [ 62,  55,  49],\n",
              "          [ 62,  51,  45],\n",
              "          ...,\n",
              "          [113,  89,  51],\n",
              "          [137, 107,  55],\n",
              "          [171, 141,  71]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 86,  82,  45],\n",
              "          [ 87,  84,  49],\n",
              "          [255, 255, 227],\n",
              "          ...,\n",
              "          [136, 119,  89],\n",
              "          [133, 117,  83],\n",
              "          [129, 113,  79]],\n",
              "\n",
              "         [[168, 162, 126],\n",
              "          [157, 153, 118],\n",
              "          [255, 255, 227],\n",
              "          ...,\n",
              "          [117, 100,  72],\n",
              "          [118, 101,  71],\n",
              "          [135, 119,  86]],\n",
              "\n",
              "         [[171, 162, 129],\n",
              "          [171, 165, 133],\n",
              "          [167, 160, 132],\n",
              "          ...,\n",
              "          [139, 124,  95],\n",
              "          [150, 133, 105],\n",
              "          [142, 125,  97]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[178, 161, 118],\n",
              "          [180, 158, 117],\n",
              "          [180, 155, 115],\n",
              "          ...,\n",
              "          [158, 111,  81],\n",
              "          [160, 117,  85],\n",
              "          [180, 139, 107]],\n",
              "\n",
              "         [[156, 141, 100],\n",
              "          [175, 153, 112],\n",
              "          [190, 162, 123],\n",
              "          ...,\n",
              "          [201, 152, 122],\n",
              "          [184, 141, 109],\n",
              "          [186, 145, 113]],\n",
              "\n",
              "         [[162, 147, 104],\n",
              "          [167, 145, 104],\n",
              "          [178, 150, 111],\n",
              "          ...,\n",
              "          [207, 160, 132],\n",
              "          [204, 159, 130],\n",
              "          [207, 164, 132]]],\n",
              "\n",
              "\n",
              "        [[[168, 140, 116],\n",
              "          [149, 123,  98],\n",
              "          [156, 130, 105],\n",
              "          ...,\n",
              "          [163, 137, 110],\n",
              "          [132, 104,  80],\n",
              "          [158, 132, 107]],\n",
              "\n",
              "         [[164, 136, 112],\n",
              "          [143, 115,  91],\n",
              "          [157, 129, 105],\n",
              "          ...,\n",
              "          [162, 136, 109],\n",
              "          [135, 109,  82],\n",
              "          [157, 131, 106]],\n",
              "\n",
              "         [[158, 128, 104],\n",
              "          [150, 120,  96],\n",
              "          [148, 120,  96],\n",
              "          ...,\n",
              "          [150, 126,  98],\n",
              "          [156, 132, 104],\n",
              "          [165, 141, 113]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[168, 143, 123],\n",
              "          [138, 113,  93],\n",
              "          [145, 120, 100],\n",
              "          ...,\n",
              "          [146, 120,  97],\n",
              "          [152, 124, 102],\n",
              "          [133, 107,  84]],\n",
              "\n",
              "         [[174, 149, 129],\n",
              "          [134, 109,  89],\n",
              "          [141, 116,  96],\n",
              "          ...,\n",
              "          [124,  98,  75],\n",
              "          [153, 122, 101],\n",
              "          [148, 120,  98]],\n",
              "\n",
              "         [[170, 145, 125],\n",
              "          [129, 104,  84],\n",
              "          [126, 101,  81],\n",
              "          ...,\n",
              "          [130, 103,  82],\n",
              "          [169, 139, 115],\n",
              "          [168, 140, 118]]],\n",
              "\n",
              "\n",
              "        [[[ 46,  55,  50],\n",
              "          [ 46,  55,  50],\n",
              "          [ 52,  61,  56],\n",
              "          ...,\n",
              "          [ 36,  45,  44],\n",
              "          [ 36,  46,  47],\n",
              "          [ 36,  46,  45]],\n",
              "\n",
              "         [[ 42,  46,  45],\n",
              "          [ 46,  50,  49],\n",
              "          [ 50,  54,  53],\n",
              "          ...,\n",
              "          [ 36,  45,  42],\n",
              "          [ 35,  44,  43],\n",
              "          [ 35,  44,  41]],\n",
              "\n",
              "         [[ 50,  56,  56],\n",
              "          [ 40,  46,  44],\n",
              "          [ 42,  48,  46],\n",
              "          ...,\n",
              "          [ 37,  46,  43],\n",
              "          [ 37,  46,  43],\n",
              "          [ 37,  46,  43]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 66, 101, 107],\n",
              "          [ 70, 105, 111],\n",
              "          [ 74, 107, 114],\n",
              "          ...,\n",
              "          [ 71, 107, 105],\n",
              "          [ 72, 106, 107],\n",
              "          [ 71, 105, 106]],\n",
              "\n",
              "         [[ 60,  95, 101],\n",
              "          [ 64,  99, 105],\n",
              "          [ 68, 101, 108],\n",
              "          ...,\n",
              "          [ 69, 105, 103],\n",
              "          [ 70, 104, 105],\n",
              "          [ 69, 103, 104]],\n",
              "\n",
              "         [[ 56,  93, 101],\n",
              "          [ 59,  96, 104],\n",
              "          [ 65, 100, 106],\n",
              "          ...,\n",
              "          [ 68, 104, 104],\n",
              "          [ 69, 103, 104],\n",
              "          [ 69, 103, 104]]]], dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[0]) #60000 training images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOAVVdI_21-l",
        "outputId": "84abac8f-af48-4f9c-cfa9-057f5690c892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Second element of train_data is the collection of image digit labels\n",
        "train_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7MVGe8sm-3f",
        "outputId": "3d8296bd-094a-4778-d489-ba9c4f37e450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 0, 4,  ..., 5, 6, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[1])   #60000 training labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU3n1HlN272w",
        "outputId": "01dc0ab0-69d7-4da7-d93c-7ea7f3b372fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing the train and test datasets.\n"
      ],
      "metadata": {
        "id": "KObSMpjEjGUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to convert array into tensors and normalize them."
      ],
      "metadata": {
        "id": "McH7Rev6j33Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG9waltm8iBE"
      },
      "outputs": [],
      "source": [
        "transform_norm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see that image shape is (28,28,3).So it is an RGB image. I converted each image to 2D grayscale image of (28,28) shape.\n",
        "\n",
        "I also generated train and test datasets where each datapoint is in (image tensor , corresponding image label) format.\n",
        "This is to get the datasets in loader object form for the CNN models."
      ],
      "metadata": {
        "id": "PuQzPjnBkMFa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkPlDxSFyeph"
      },
      "outputs": [],
      "source": [
        "train=[]\n",
        "for i in range(len(train_data[1])):\n",
        "  arr=np.array(train_data[0][i])\n",
        "  #grayscaling using opencv\n",
        "  arr=cv2.cvtColor(arr,cv2.COLOR_BGR2GRAY)\n",
        "  #appending (image tensor , corresponding image label) tuples.\n",
        "  train.append((transform_norm(arr),int(train_data[1][i])))\n",
        "#converting list to tuple\n",
        "train=tuple(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBjVC0zO0IHt"
      },
      "outputs": [],
      "source": [
        "#Same goes for the test dataset\n",
        "test=[]\n",
        "for i in range(len(test_data[1])):\n",
        "  arr=np.array(test_data[0][i])\n",
        "  arr=cv2.cvtColor(arr,cv2.COLOR_BGR2GRAY)\n",
        "  test.append((transform_norm(arr),int(test_data[1][i])))\n",
        "test=tuple(test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of samples in the train set :{len(train)}\")\n",
        "print(f\"The number of samples in the test set :{len(test)}\")\n",
        "print(f\"The shape of each image tensor in both train and test  :{train[0][0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4D5QYSnoRRN",
        "outputId": "bdd6b9ba-777d-43d2-99f0-42630ca39ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of samples in the train set :60000\n",
            "The number of samples in the test set :10000\n",
            "The shape of each image tensor in both train and test  :torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a deep CNN model.\n",
        "\n",
        "\n",
        "Now for this I have tried many different models. I have only shown the model with the highest accuracy and didn't show any other models.That is because I don't want to make this notebook messy.These are my observations on the accuracy for different models:\n",
        "1.   LSTM : Approximately 86% accracy over 10 epochs.\n",
        "2.   RNNs : Approximately 81% accracy over 10 epochs.\n",
        "3.   Bidirectional LSTM : Approximately 88% accracy over 10 epochs.\n",
        "4.   Simple CNN : Approximately 83% accracy over 10 epochs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J0nowqYVpv9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ModelM3 CNN model\n",
        "\n",
        "This is one of the models proposed in this research paper : [https://arxiv.org/abs/2008.10400](https://)\n",
        "\n",
        "(Note that in this paper they have proposed a much more complex model but due to time limitions I didn't implement this research paper as it is. Instead I have used it to implement some parts of my model.)\n",
        "\n",
        "Here is a brief introduction to the model proposed in the paper: \n",
        " This network models consist of multiple convolution layers(10) and 2 fully connected layer at the end. In each convolution layer, a 2D convolution is performed, followed by a 2D batch normalization and ReLU activation. Max pooling or average pooling is not used after convolution. Instead, the size of feature map is reduced after each convolution because padding is not used.I have used a 3Ã—3 kernel,so the width and height of the image is reduced by two after each convolution layer.\n",
        "\n",
        " In order to avoid the problem of overfitting I have also added a dropout layers before each fully connected layer. Dropout rates are calculated using hyperparameter tuning."
      ],
      "metadata": {
        "id": "udWNKT27taBs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP1mkgS41mmR"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Hyper-parameters \n",
        "# input_size = 784 # 28x28\n",
        "num_classes = 10\n",
        "input_size = 784"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhUphPWxpdFz"
      },
      "outputs": [],
      "source": [
        "class ModelM3(nn.Module):\n",
        "    def __init__(self,trial):\n",
        "        super(ModelM3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, bias=False)       # output becomes 26x26\n",
        "        self.conv1_bn = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 48, 3, bias=False)      # output becomes 24x24\n",
        "        self.conv2_bn = nn.BatchNorm2d(48)\n",
        "        self.conv3 = nn.Conv2d(48, 64, 3, bias=False)      # output becomes 22x22\n",
        "        self.conv3_bn = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 80, 3, bias=False)      # output becomes 20x20\n",
        "        self.conv4_bn = nn.BatchNorm2d(80)\n",
        "        self.conv5 = nn.Conv2d(80, 96, 3, bias=False)      # output becomes 18x18\n",
        "        self.conv5_bn = nn.BatchNorm2d(96)\n",
        "        self.conv6 = nn.Conv2d(96, 112, 3, bias=False)     # output becomes 16x16\n",
        "        self.conv6_bn = nn.BatchNorm2d(112)\n",
        "        self.conv7 = nn.Conv2d(112, 128, 3, bias=False)    # output becomes 14x14\n",
        "        self.conv7_bn = nn.BatchNorm2d(128)\n",
        "        self.conv8 = nn.Conv2d(128, 144, 3, bias=False)    # output becomes 12x12\n",
        "        self.conv8_bn = nn.BatchNorm2d(144)\n",
        "        self.conv9 = nn.Conv2d(144, 160, 3, bias=False)    # output becomes 10x10\n",
        "        self.conv9_bn = nn.BatchNorm2d(160)\n",
        "        self.conv10 = nn.Conv2d(160, 176, 3, bias=False)   # output becomes 8x8\n",
        "        self.conv10_bn = nn.BatchNorm2d(176)\n",
        "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0, 0.5,step=0.1)\n",
        "        self.drop1=nn.Dropout2d(p=dropout_rate)   \n",
        "        fc2_input_dim =  trial.suggest_int(\"fc2_input_dim\", 32, 128,32)\n",
        "        self.fc1 = nn.Linear( 11264,fc2_input_dim)\n",
        "        dropout_rate2 = trial.suggest_float(\"dropout_rate2\", 0, 0.3,step=0.1)\n",
        "        self.drop2=nn.Dropout2d(p=dropout_rate2)\n",
        "        self.fc2 = nn.Linear(fc2_input_dim, 10)\n",
        "        self.fc1_bn = nn.BatchNorm1d(10)\n",
        "    def forward(self, x):\n",
        "        conv1 = F.relu(self.conv1_bn(self.conv1(x)))\n",
        "        conv2 = F.relu(self.conv2_bn(self.conv2(conv1)))\n",
        "        conv3 = F.relu(self.conv3_bn(self.conv3(conv2)))\n",
        "        conv4 = F.relu(self.conv4_bn(self.conv4(conv3)))\n",
        "        conv5 = F.relu(self.conv5_bn(self.conv5(conv4)))\n",
        "        conv6 = F.relu(self.conv6_bn(self.conv6(conv5)))\n",
        "        conv7 = F.relu(self.conv7_bn(self.conv7(conv6)))\n",
        "        conv8 = F.relu(self.conv8_bn(self.conv8(conv7)))\n",
        "        conv9 = F.relu(self.conv9_bn(self.conv9(conv8)))\n",
        "        conv10 = F.relu(self.conv10_bn(self.conv10(conv9)))\n",
        "        x=  self.drop1(conv10)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning the CNN model using Optuna package\n",
        "\n",
        "Here we will optimize following hyperparameters :\n",
        "1.   Batch size\n",
        "2.   Dropout_rate1 for first conneted layer.\n",
        "3.   Dropout_rate2 for second conneted layer.\n",
        "4.   fc2_input_dim is the number of output channels for first connected layer\n",
        "\n",
        "\n",
        "For this we are using a Optuna package. In Optuna, the goal is to minimize/maximize the objective function, which takes as input a set of hyperparameters and returns a validation score. For each hyperparameter, we consider a different range of values.\n"
      ],
      "metadata": {
        "id": "-EHyk0SWvnTc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7IP0RVQSBfY"
      },
      "outputs": [],
      "source": [
        "# objective function takes trial as an input which contains optimizer,batch_size,learning_rate\n",
        "def objective(trial):\n",
        "\n",
        "    # Generate the model.\n",
        "    model = ModelM3(trial).to(DEVICE)\n",
        "\n",
        "    # Generate the optimizers.\n",
        "\n",
        "    # try RMSprop and SGD\n",
        "    '''\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\"])\n",
        "    momentum = trial.suggest_float(\"momentum\", 0.0, 1.0)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr,momentum=momentum)\n",
        "    '''\n",
        "    #try Adam, AdaDelta adn Adagrad\n",
        "    \n",
        "    #Defining search space for each parameter.\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"Adadelta\",\"Adagrad\"])  \n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1,log=True)              \n",
        "    batch_size=trial.suggest_int(\"batch_size\", 64, 256,step=64)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "    # Converting train and test datasets we obtsained before into dataloader object.\n",
        "    train_loader=torch.utils.data.DataLoader(train,shuffle=True,batch_size=batch_size,pin_memory=True)\n",
        "    test_loader=torch.utils.data.DataLoader(test,shuffle=False,batch_size=batch_size,pin_memory=True)\n",
        "    \n",
        "    # Training of the model.\n",
        "    EPOCHS=5\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()   \n",
        "       \n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            # Limiting training images for faster epochs.\n",
        "            #if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
        "            #    break\n",
        "\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()    #backpropogation \n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation of the model.\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "                # Limiting validation images.\n",
        "               # if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
        "                #    break\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                output = model(images)\n",
        "                # Get the index of the max log-probability.\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "\n",
        "        accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Somethings related to the code below-\n",
        "\n",
        "\n",
        "*  Sampler : Sampler value indicates which sampler method you want Optuna to implement. I am using TPESampler() which is based on Bayesian hyperparameter optimization, which is an efficient method for hyperparameter tuning.\n",
        "*   n_trials : Indicates the number of trials in a study that you want Optuna to perform, here it's 20.\n",
        "*   optimize method : Optimize method in our study will optimize the output of the function objective i.e accuracy of the model.\n",
        "\n",
        "  At the end we will choose the hyperparameters that give us the best accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hcCe3qiO09nw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZlvv1-TTKLm",
        "outputId": "fe9efcb0-6de2-4c0b-d0ff-992c581ecd1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-04 08:42:09,076]\u001b[0m A new study created in memory with name: no-name-d86ac46f-0200-4b50-9e8d-d543f0ac802d\u001b[0m\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n",
            "\u001b[32m[I 2022-12-04 08:49:36,760]\u001b[0m Trial 0 finished with value: 0.8721 and parameters: {'dropout_rate': 0.5, 'fc2_input_dim': 128, 'dropout_rate2': 0.0, 'optimizer': 'Adam', 'lr': 0.00011600367730872151, 'batch_size': 192}. Best is trial 0 with value: 0.8721.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 08:56:42,305]\u001b[0m Trial 1 finished with value: 0.8094 and parameters: {'dropout_rate': 0.0, 'fc2_input_dim': 64, 'dropout_rate2': 0.1, 'optimizer': 'Adagrad', 'lr': 0.0010552634425746344, 'batch_size': 128}. Best is trial 0 with value: 0.8721.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:03:55,308]\u001b[0m Trial 2 finished with value: 0.7309 and parameters: {'dropout_rate': 0.2, 'fc2_input_dim': 96, 'dropout_rate2': 0.2, 'optimizer': 'Adam', 'lr': 0.02971330779861973, 'batch_size': 192}. Best is trial 0 with value: 0.8721.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:11:01,408]\u001b[0m Trial 3 finished with value: 0.8627 and parameters: {'dropout_rate': 0.4, 'fc2_input_dim': 32, 'dropout_rate2': 0.3, 'optimizer': 'Adadelta', 'lr': 0.07993321459889022, 'batch_size': 64}. Best is trial 0 with value: 0.8721.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:18:09,940]\u001b[0m Trial 4 finished with value: 0.5559 and parameters: {'dropout_rate': 0.0, 'fc2_input_dim': 96, 'dropout_rate2': 0.2, 'optimizer': 'Adagrad', 'lr': 0.0002144143004682391, 'batch_size': 64}. Best is trial 0 with value: 0.8721.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:18:52,204]\u001b[0m Trial 5 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:25:52,780]\u001b[0m Trial 6 finished with value: 0.8681 and parameters: {'dropout_rate': 0.4, 'fc2_input_dim': 32, 'dropout_rate2': 0.1, 'optimizer': 'Adam', 'lr': 0.004975124465597255, 'batch_size': 64}. Best is trial 0 with value: 0.8721.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:26:35,489]\u001b[0m Trial 7 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:27:16,960]\u001b[0m Trial 8 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:28:00,629]\u001b[0m Trial 9 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:28:44,424]\u001b[0m Trial 10 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:36:03,511]\u001b[0m Trial 11 finished with value: 0.8893 and parameters: {'dropout_rate': 0.4, 'fc2_input_dim': 128, 'dropout_rate2': 0.1, 'optimizer': 'Adam', 'lr': 0.00021710064969469495, 'batch_size': 256}. Best is trial 11 with value: 0.8893.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:43:23,642]\u001b[0m Trial 12 finished with value: 0.8821 and parameters: {'dropout_rate': 0.30000000000000004, 'fc2_input_dim': 128, 'dropout_rate2': 0.0, 'optimizer': 'Adam', 'lr': 0.00014192267774525692, 'batch_size': 256}. Best is trial 11 with value: 0.8893.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:50:42,496]\u001b[0m Trial 13 finished with value: 0.8931 and parameters: {'dropout_rate': 0.30000000000000004, 'fc2_input_dim': 128, 'dropout_rate2': 0.1, 'optimizer': 'Adam', 'lr': 0.00036062684994419414, 'batch_size': 256}. Best is trial 13 with value: 0.8931.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 09:57:57,138]\u001b[0m Trial 14 finished with value: 0.8982 and parameters: {'dropout_rate': 0.30000000000000004, 'fc2_input_dim': 96, 'dropout_rate2': 0.1, 'optimizer': 'Adam', 'lr': 0.00048437699638469595, 'batch_size': 256}. Best is trial 14 with value: 0.8982.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 10:05:12,873]\u001b[0m Trial 15 finished with value: 0.8959 and parameters: {'dropout_rate': 0.30000000000000004, 'fc2_input_dim': 96, 'dropout_rate2': 0.1, 'optimizer': 'Adam', 'lr': 0.00043260083929742615, 'batch_size': 256}. Best is trial 14 with value: 0.8982.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 10:05:56,532]\u001b[0m Trial 16 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-12-04 10:13:10,322]\u001b[0m Trial 17 finished with value: 0.9014 and parameters: {'dropout_rate': 0.30000000000000004, 'fc2_input_dim': 96, 'dropout_rate2': 0.2, 'optimizer': 'Adam', 'lr': 0.0006929852280913366, 'batch_size': 256}. Best is trial 17 with value: 0.9014.\u001b[0m\n",
            "\u001b[32m[I 2022-12-04 10:13:54,333]\u001b[0m Trial 18 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-12-04 10:14:36,757]\u001b[0m Trial 19 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9014\n",
            "Best hyperparameters: {'dropout_rate': 0.30000000000000004, 'fc2_input_dim': 96, 'dropout_rate2': 0.2, 'optimizer': 'Adam', 'lr': 0.0006929852280913366, 'batch_size': 256}\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction='maximize',sampler=TPESampler())\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "trial = study.best_trial\n",
        "\n",
        "print('Accuracy: {}'.format(trial.value))\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the best performing hyperparameters to increase our accuracy are:\n",
        "\n",
        "1.   dropout_rate1 :    0.30000000000000004\n",
        "2.   fc2_input_dim :    96\n",
        "3.   dropout_rate2 : 0.2\n",
        "4.   optimizer:  Adam\n",
        "5.   batch_size:  256\n",
        "\n"
      ],
      "metadata": {
        "id": "bb7wdwd84jn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I managed to obtain the best hyperparameters that will maximize our model accuracy. Also for tuning I decided to use small number of EPOCHS(only 2) since it was taking too long to run. But now we can run the model on higher number of EPOCHS using the hyperparameters we obtained above.So let's rebuild the model again by substituting abovementioned hyperparameters.**"
      ],
      "metadata": {
        "id": "a-SGGTfE3xlB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj_ND6o-EL-5"
      },
      "outputs": [],
      "source": [
        "class ModelM3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelM3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, bias=False)       # output becomes 26x26\n",
        "        self.conv1_bn = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 48, 3, bias=False)      # output becomes 24x24\n",
        "        self.conv2_bn = nn.BatchNorm2d(48)\n",
        "        self.conv3 = nn.Conv2d(48, 64, 3, bias=False)      # output becomes 22x22\n",
        "        self.conv3_bn = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 80, 3, bias=False)      # output becomes 20x20\n",
        "        self.conv4_bn = nn.BatchNorm2d(80)\n",
        "        self.conv5 = nn.Conv2d(80, 96, 3, bias=False)      # output becomes 18x18\n",
        "        self.conv5_bn = nn.BatchNorm2d(96)\n",
        "        self.conv6 = nn.Conv2d(96, 112, 3, bias=False)     # output becomes 16x16\n",
        "        self.conv6_bn = nn.BatchNorm2d(112)\n",
        "        self.conv7 = nn.Conv2d(112, 128, 3, bias=False)    # output becomes 14x14\n",
        "        self.conv7_bn = nn.BatchNorm2d(128)\n",
        "        self.conv8 = nn.Conv2d(128, 144, 3, bias=False)    # output becomes 12x12\n",
        "        self.conv8_bn = nn.BatchNorm2d(144)\n",
        "        self.conv9 = nn.Conv2d(144, 160, 3, bias=False)    # output becomes 10x10\n",
        "        self.conv9_bn = nn.BatchNorm2d(160)\n",
        "        self.conv10 = nn.Conv2d(160, 176, 3, bias=False)   # output becomes 8x8\n",
        "        self.conv10_bn = nn.BatchNorm2d(176)\n",
        "        dropout_rate =  0.30000000000000004\n",
        "        self.drop1=nn.Dropout2d(p=dropout_rate)   \n",
        "        fc2_input_dim =96\n",
        "        self.fc1 = nn.Linear( 11264,fc2_input_dim)\n",
        "        dropout_rate2=0.2\n",
        "        self.drop2=nn.Dropout2d(p=dropout_rate2)\n",
        "        self.fc2 = nn.Linear(fc2_input_dim, 10)\n",
        "        self.fc1_bn = nn.BatchNorm1d(10)\n",
        "    def forward(self, x):\n",
        "        conv1 = F.relu(self.conv1_bn(self.conv1(x)))\n",
        "        conv2 = F.relu(self.conv2_bn(self.conv2(conv1)))\n",
        "        conv3 = F.relu(self.conv3_bn(self.conv3(conv2)))\n",
        "        conv4 = F.relu(self.conv4_bn(self.conv4(conv3)))\n",
        "        conv5 = F.relu(self.conv5_bn(self.conv5(conv4)))\n",
        "        conv6 = F.relu(self.conv6_bn(self.conv6(conv5)))\n",
        "        conv7 = F.relu(self.conv7_bn(self.conv7(conv6)))\n",
        "        conv8 = F.relu(self.conv8_bn(self.conv8(conv7)))\n",
        "        conv9 = F.relu(self.conv9_bn(self.conv9(conv8)))\n",
        "        conv10 = F.relu(self.conv10_bn(self.conv10(conv9)))\n",
        "        x=  self.drop1(conv10)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training and Evaluation."
      ],
      "metadata": {
        "id": "BrjRiSehUm4e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783a8280-d908-4fd7-a0bb-eec231dbeaef",
        "id": "JW7rnqE-ypyl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15], Step [100/235],Loss: 0.4802\n",
            "Epoch [1/15], Step [200/235],Loss: 0.4044\n",
            "Accuracy of the network on the 10000 test images: 90.11 %\n",
            "Epoch [2/15], Step [100/235],Loss: 0.3335\n",
            "Epoch [2/15], Step [200/235],Loss: 0.3554\n",
            "Accuracy of the network on the 10000 test images: 93.84 %\n",
            "Epoch [3/15], Step [100/235],Loss: 0.2055\n",
            "Epoch [3/15], Step [200/235],Loss: 0.2271\n",
            "Accuracy of the network on the 10000 test images: 95.45 %\n",
            "Epoch [4/15], Step [100/235],Loss: 0.2351\n",
            "Epoch [4/15], Step [200/235],Loss: 0.1671\n",
            "Accuracy of the network on the 10000 test images: 95.54 %\n",
            "Epoch [5/15], Step [100/235],Loss: 0.0900\n",
            "Epoch [5/15], Step [200/235],Loss: 0.1374\n",
            "Accuracy of the network on the 10000 test images: 95.82000000000001 %\n",
            "Epoch [6/15], Step [100/235],Loss: 0.1167\n",
            "Epoch [6/15], Step [200/235],Loss: 0.1041\n",
            "Accuracy of the network on the 10000 test images: 95.98 %\n",
            "Epoch [7/15], Step [100/235],Loss: 0.0763\n",
            "Epoch [7/15], Step [200/235],Loss: 0.1539\n",
            "Accuracy of the network on the 10000 test images: 96.14 %\n",
            "Epoch [8/15], Step [100/235],Loss: 0.0772\n",
            "Epoch [8/15], Step [200/235],Loss: 0.0734\n",
            "Accuracy of the network on the 10000 test images: 96.83 %\n",
            "Epoch [9/15], Step [100/235],Loss: 0.0993\n",
            "Epoch [9/15], Step [200/235],Loss: 0.1044\n",
            "Accuracy of the network on the 10000 test images: 96.58 %\n",
            "Epoch [10/15], Step [100/235],Loss: 0.0541\n",
            "Epoch [10/15], Step [200/235],Loss: 0.1223\n",
            "Accuracy of the network on the 10000 test images: 96.7 %\n",
            "Epoch [11/15], Step [100/235],Loss: 0.0936\n",
            "Epoch [11/15], Step [200/235],Loss: 0.0819\n",
            "Accuracy of the network on the 10000 test images: 96.39 %\n",
            "Epoch [12/15], Step [100/235],Loss: 0.0449\n",
            "Epoch [12/15], Step [200/235],Loss: 0.1137\n",
            "Accuracy of the network on the 10000 test images: 96.31 %\n",
            "Epoch [13/15], Step [100/235],Loss: 0.0621\n",
            "Epoch [13/15], Step [200/235],Loss: 0.1204\n",
            "Accuracy of the network on the 10000 test images: 96.77 %\n",
            "Epoch [14/15], Step [100/235],Loss: 0.1153\n",
            "Epoch [14/15], Step [200/235],Loss: 0.0510\n",
            "Accuracy of the network on the 10000 test images: 96.44 %\n",
            "Epoch [15/15], Step [100/235],Loss: 0.1169\n",
            "Epoch [15/15], Step [200/235],Loss: 0.0726\n",
            "Accuracy of the network on the 10000 test images: 96.95 %\n"
          ]
        }
      ],
      "source": [
        "model = ModelM3().to(DEVICE)\n",
        "learning_rate = 0.0006929852280913366\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "batch_size=256\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "#define train_loader and test_loader objects.\n",
        "train_loader=torch.utils.data.DataLoader(train,shuffle=True,batch_size=batch_size,pin_memory=True)\n",
        "test_loader=torch.utils.data.DataLoader(test,shuffle=False,batch_size=batch_size,pin_memory=True)\n",
        "    \n",
        "# Training of the model.\n",
        "EPOCHS=15\n",
        "n_total_steps = len(train_loader) \n",
        "for epoch in range(EPOCHS):\n",
        "      model.train()\n",
        "       \n",
        "      for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()    #backpropogation\n",
        "            optimizer.step()\n",
        "            if (batch_idx+1) % 100 == 0:\n",
        "                  print (f'Epoch [{epoch+1}/{EPOCHS}], Step [{batch_idx+1}/{n_total_steps}],Loss: {loss.item():.4f}')\n",
        "      #Evaluation on test dataset.\n",
        "      model.eval()\n",
        "      correct = 0\n",
        "      with torch.no_grad():\n",
        "          for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "                # Limiting validation images.\n",
        "               # if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
        "                #    break\n",
        "              images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "              output = model(images)\n",
        "               # Get the index of the max log-probability.\n",
        "              pred = output.argmax(dim=1, keepdim=True)\n",
        "              correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "\n",
        "      accuracy = (correct / len(test_loader.dataset))*100\n",
        "      print(f'Accuracy of the network on the 10000 test images: {accuracy} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As you can see this model is getting almost 97% accuracy on the 10000 test images.**"
      ],
      "metadata": {
        "id": "9FIZgQiCJBed"
      }
    },
    {
      "metadata": {
        "id": "pCQpHoC_pCX3"
      },
      "cell_type": "markdown",
      "source": [
        "## Saving and loading networks\n",
        "\n",
        "Most of the times these models take too much time to train. \n",
        "So it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
        "\n",
        "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers.Same goes for optimizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Our model: \\n\\n\", model, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaMIezRZzrlP",
        "outputId": "4342bd50-136f-4684-cc6c-1c84460d90c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model: \n",
            "\n",
            " ModelM3(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv2_bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv4_bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(80, 96, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv5_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv6): Conv2d(96, 112, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv6_bn): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv7): Conv2d(112, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv7_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv8): Conv2d(128, 144, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv8_bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv9): Conv2d(144, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv9_bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv10): Conv2d(160, 176, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "  (conv10_bn): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (drop1): Dropout2d(p=0.30000000000000004, inplace=False)\n",
            "  (fc1): Linear(in_features=11264, out_features=96, bias=True)\n",
            "  (drop2): Dropout2d(p=0.2, inplace=False)\n",
            "  (fc2): Linear(in_features=96, out_features=10, bias=True)\n",
            "  (fc1_bn): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " odict_keys(['conv1.weight', 'conv1_bn.weight', 'conv1_bn.bias', 'conv1_bn.running_mean', 'conv1_bn.running_var', 'conv1_bn.num_batches_tracked', 'conv2.weight', 'conv2_bn.weight', 'conv2_bn.bias', 'conv2_bn.running_mean', 'conv2_bn.running_var', 'conv2_bn.num_batches_tracked', 'conv3.weight', 'conv3_bn.weight', 'conv3_bn.bias', 'conv3_bn.running_mean', 'conv3_bn.running_var', 'conv3_bn.num_batches_tracked', 'conv4.weight', 'conv4_bn.weight', 'conv4_bn.bias', 'conv4_bn.running_mean', 'conv4_bn.running_var', 'conv4_bn.num_batches_tracked', 'conv5.weight', 'conv5_bn.weight', 'conv5_bn.bias', 'conv5_bn.running_mean', 'conv5_bn.running_var', 'conv5_bn.num_batches_tracked', 'conv6.weight', 'conv6_bn.weight', 'conv6_bn.bias', 'conv6_bn.running_mean', 'conv6_bn.running_var', 'conv6_bn.num_batches_tracked', 'conv7.weight', 'conv7_bn.weight', 'conv7_bn.bias', 'conv7_bn.running_mean', 'conv7_bn.running_var', 'conv7_bn.num_batches_tracked', 'conv8.weight', 'conv8_bn.weight', 'conv8_bn.bias', 'conv8_bn.running_mean', 'conv8_bn.running_var', 'conv8_bn.num_batches_tracked', 'conv9.weight', 'conv9_bn.weight', 'conv9_bn.bias', 'conv9_bn.running_mean', 'conv9_bn.running_var', 'conv9_bn.num_batches_tracked', 'conv10.weight', 'conv10_bn.weight', 'conv10_bn.bias', 'conv10_bn.running_mean', 'conv10_bn.running_var', 'conv10_bn.num_batches_tracked', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc1_bn.weight', 'fc1_bn.bias', 'fc1_bn.running_mean', 'fc1_bn.running_var', 'fc1_bn.num_batches_tracked'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Our optimizer : \\n\\n\", optimizer, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", optimizer.state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSKeVTC4GG5X",
        "outputId": "dd06f9df-172f-473f-c2e9-4fb72d584e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our optimizer : \n",
            "\n",
            " Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " dict_keys(['state', 'param_groups'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "            }, 'checkpoint.pt')\n",
        "from google.colab import files\n",
        "# download checkpoint file\n",
        "files.download('checkpoint.pt')"
      ],
      "metadata": {
        "id": "XQ0FPPhAzxxa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9e9d7375-8ce0-41c5-ece8-f71acd2feaf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6a76cf26-be5b-480d-b11f-79c0dc0b1657\", \"Checkpoint.pt\", 25183275)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15fFER0RpqIyivhgS2z1WMMOujeeA7eEI",
      "authorship_tag": "ABX9TyPpLcBHtGlWaV8W87zRtUZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "617c4cc74d004ef382800e5600a831c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_befe02bf9d3a48e0b21a662a394ded31",
              "IPY_MODEL_9a3f8f5a600543dea4f7503eab169192",
              "IPY_MODEL_f64a9a3ad6994d3a8f649f7aadfec484"
            ],
            "layout": "IPY_MODEL_5d6fa4c2ff084a91a530d4a341c5fc62"
          }
        },
        "befe02bf9d3a48e0b21a662a394ded31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a51a8559249249e1b85d0043b925d0b3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_22eb1b86b5a14dd6ad7868c697820f99",
            "value": "100%"
          }
        },
        "9a3f8f5a600543dea4f7503eab169192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80e9f608fda94193b487b0233fe2f1f0",
            "max": 112143175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f85549714af4f4b9d0116e3c614614e",
            "value": 112143175
          }
        },
        "f64a9a3ad6994d3a8f649f7aadfec484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_005ecfa5731e48eb895b2bcdfe41bdce",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_866ea2b99508436b99d74d83e0a28237",
            "value": " 112143175/112143175 [00:22&lt;00:00, 6141046.09it/s]"
          }
        },
        "5d6fa4c2ff084a91a530d4a341c5fc62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a51a8559249249e1b85d0043b925d0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22eb1b86b5a14dd6ad7868c697820f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80e9f608fda94193b487b0233fe2f1f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f85549714af4f4b9d0116e3c614614e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "005ecfa5731e48eb895b2bcdfe41bdce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "866ea2b99508436b99d74d83e0a28237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ca69999cbb443b3ba5c167cd004d774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04d79a3b1d3c43e9aec7b8b486356c3c",
              "IPY_MODEL_0080f9ff5e124d1f853ad84998862177",
              "IPY_MODEL_a550b262059f4fd884cfbb5ddb861988"
            ],
            "layout": "IPY_MODEL_c93e810133a14a268c416f6d572e2f01"
          }
        },
        "04d79a3b1d3c43e9aec7b8b486356c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca2b4c09a3754fd182975cb838ad6b27",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cb82bfd4a40d484790cb5f79276d4a12",
            "value": "100%"
          }
        },
        "0080f9ff5e124d1f853ad84998862177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd3079cbc19f446d8d6dfa9b832e3772",
            "max": 18663594,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcb1ee9fc47d41ad9271abff8d25ba69",
            "value": 18663594
          }
        },
        "a550b262059f4fd884cfbb5ddb861988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_850abcd3bdc744c69fb4f282af5e20ad",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_359f20001b2043c0b2723b08bdc5b337",
            "value": " 18663594/18663594 [00:01&lt;00:00, 14249083.55it/s]"
          }
        },
        "c93e810133a14a268c416f6d572e2f01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca2b4c09a3754fd182975cb838ad6b27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb82bfd4a40d484790cb5f79276d4a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd3079cbc19f446d8d6dfa9b832e3772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcb1ee9fc47d41ad9271abff8d25ba69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "850abcd3bdc744c69fb4f282af5e20ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "359f20001b2043c0b2723b08bdc5b337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}